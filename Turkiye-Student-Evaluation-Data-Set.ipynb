{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkiye-Student-Evaluation-Data-Set\n",
    "\n",
    "## Problem Statement:-\n",
    "\n",
    " In this project we are basically going to perform clustering on the given data and these clusters will signify different categories of students based on the marks, content, course and other features. The clustering algorithm used in the project is K-Means Clustering. The aim is to cluster the data on the basis of the given features, which will ultimately cluster together the student's with similar performance.\n",
    "\n",
    "## Attribute Information:-\n",
    "\n",
    "The dataset has  5820 instances with 33 attributes. The description of each column is given below:\n",
    "\n",
    "instr: Instructor's identifier; values taken from {1,2,3}\n",
    "\n",
    "class: Course code (descriptor); values taken from {1-13}\n",
    "\n",
    "repeat: Number of times the student is taking this course; values taken from {0,1,2,3,...}\n",
    "\n",
    "attendance: Code of the level of attendance; values from {0, 1, 2, 3, 4}\n",
    "\n",
    "difficulty: Level of difficulty of the course as perceived by the student; values taken from {1,2,3,4,5}\n",
    "\n",
    "Q1: The semester course content, teaching method and evaluation system were provided at the start.\n",
    "\n",
    "Q2: The course aims and objectives were clearly stated at the beginning of the period.\n",
    "\n",
    "Q3: The course was worth the amount of credit assigned to it.\n",
    "\n",
    "Q4: The course was taught according to the syllabus announced on the first day of class.\n",
    "\n",
    "Q5: The class discussions, homework assignments, applications and studies were satisfactory.\n",
    "\n",
    "Q6: The textbook and other courses resources were sufficient and up to date.\n",
    "\n",
    "Q7: The course allowed field work, applications, laboratory, discussion and other studies.\n",
    "\n",
    "Q8: The quizzes, assignments, projects and exams contributed to helping the learning.\n",
    "\n",
    "Q9: I greatly enjoyed the class and was eager to actively participate during the lectures.\n",
    "\n",
    "Q10: My initial expectations about the course were met at the end of the period or year.\n",
    "\n",
    "Q11: The course was relevant and beneficial to my professional development.\n",
    "\n",
    "Q12: The course helped me look at life and the world with a new perspective.\n",
    "\n",
    "Q13: The Instructor's knowledge was relevant and up to date.\n",
    "\n",
    "Q14: The Instructor came prepared for classes.\n",
    "\n",
    "Q15: The Instructor taught in accordance with the announced lesson plan.\n",
    "\n",
    "Q16: The Instructor was committed to the course and was understandable.\n",
    "\n",
    "Q17: The Instructor arrived on time for classes.\n",
    "\n",
    "Q18: The Instructor has a smooth and easy to follow delivery/speech.\n",
    "\n",
    "Q19: The Instructor made effective use of class hours.\n",
    "\n",
    "Q20: The Instructor explained the course and was eager to be helpful to students.\n",
    "\n",
    "Q21: The Instructor demonstrated a positive approach to students.\n",
    "\n",
    "Q22: The Instructor was open and respectful of the views of students about the course.\n",
    "\n",
    "Q23: The Instructor encouraged participation in the course.\n",
    "\n",
    "Q24: The Instructor gave relevant homework assignments/projects, and helped/guided students.\n",
    "\n",
    "Q25: The Instructor responded to questions about the course inside and outside of the course.\n",
    "\n",
    "Q26: The Instructor's evaluation system (midterm and final questions, projects, assignments, etc.) effectively measured the course objectives.\n",
    "\n",
    "Q27: The Instructor provided solutions to exams and discussed them with students.\n",
    "\n",
    "Q28: The Instructor treated all students in a right and objective manner.\n",
    "\n",
    "Q1-Q28 are all Likert-type, meaning that the values are taken from {1,2,3,4,5}\n",
    "\n",
    "## We will be implementing the following steps to achieve the final result:-\n",
    "\n",
    "1. Importing the necessary Libraries.\n",
    "\n",
    "2. Importing the dataset.\n",
    "\n",
    "3. Performing feature engineering i.e. modifying existing variables and creating new ones for analysis.\n",
    "\n",
    "4. Building The model.\n",
    "\n",
    "5. Visualising the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np for processing data\n",
    "\n",
    "\n",
    "# import pandas as pd for importing the data and working with data\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt for visualisation\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler for data preprocessing \n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import normalize for normalizing your data\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import PCA for dimensionality reduction\n",
    "\n",
    "\n",
    "# from sklearn.cluster import KMeans for implementing the clustering algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.read_csv('filename.csv') to import the necessary file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using data.head() we can see the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the shape and size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df.shape() and df.size() will give you the shape and size of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df.describe() will describe the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the non null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df.info() we can see number of non null values present\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using df.columns we can see the list of all the columns in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Performing Feature Engineering\n",
    "\n",
    "We will be performing the following 3 steps:\n",
    "\n",
    "1.Standard Scaler\n",
    "\n",
    "2.Normalization\n",
    "\n",
    "3.Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler:\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "\n",
    "For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "where u is the mean of the training samples, and s is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the standard scaler first create an object for StandardScaler\n",
    "\n",
    "\n",
    "# now perform scaler.fit() to fit the data\n",
    "\n",
    "\n",
    "# now perform scaler.transform() to get the scaled data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization:\n",
    "\n",
    "Normalization is used to scale the data of an attribute so that it falls in a smaller range, such as -1.0 to 1.0 or 0.0 to 1.0. It is generally useful for classification algorithms.\n",
    "\n",
    "Normalization is generally required when we are dealing with attributes on a different scale, otherwise, it may lead to a dilution in effectiveness of an important equally important attribute(on lower scale) because of other attribute having values on larger scale.\n",
    "In simple words, when multiple attributes are there but attributes have values on different scales, this may lead to poor data models while performing data mining operations. So they are normalized to bring all the attributes on the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now implement the normalization to normalize the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Principal Components Analysis is an unsupervised learning class of statistical techniques used to explain data in high dimension using smaller number of variables called the principal components.\n",
    "\n",
    "Assuming we have a set X made up of n measurements each represented by a set of p features, X1, X2, … , Xp. If we want to plot this data in a 2-dimensional plane, we can plot n measurements using two features at a time. If the number of features are more than 3 or four then plotting this in two dimension will be a challenge as the number of plots would be p(p-1)/2 which would be hard to plot.\n",
    "We would like to visualize this data in two dimension without losing information contained in the data. This is what PCA allows us to do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so to implement PCA first we need to create an object for PCA and also need to mention that how many dimensions we need finally\n",
    "\n",
    "\n",
    "# now do pca.fit(data) to fit the data\n",
    "\n",
    "\n",
    "# now do pca.transform(data) to transform the higher-dimensionality data to lower dimensions\n",
    "\n",
    "\n",
    "# now after implementing pca, use pd.DataFrame(data) to convert the new data into a Data Frame else\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Building The model\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. First we initialize k points, called means, randomly.\n",
    "\n",
    "2. We categorize each item to its closest mean and we update the mean’s coordinates, which are the averages of the items categorized in that mean so far.\n",
    "\n",
    "3. We repeat the process for a given number of iterations and at the end, we have our clusters.\n",
    "\n",
    "So, basically we will be following two steps:-\n",
    "\n",
    "1. Implementing the Elbow method which we will return the optimal value of clusters to be formed.\n",
    "\n",
    "2. We will implement K-Means algorithm to create the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Elbow Method\n",
    "\n",
    "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use. The same method can be used to choose the number of parameters in other data-driven models, such as the number of principal components to describe a data set.\n",
    "\n",
    "A fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.\n",
    "\n",
    "To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement the elbow method first create an empty list name it as wcss\n",
    "\n",
    "\n",
    "# now initiate a for loop ranging between (1,11) and implement k-means clustering for every i number of clusters\n",
    "\n",
    "\n",
    "# keep appending the empty list with the kmeans.inertia_ values \n",
    "\n",
    "\n",
    "# now plot the graph between the range(1,11) and the wcss \n",
    "\n",
    "\n",
    "# from the plot we determine the optimal value of k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-Means:\n",
    "\n",
    "K-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. It is popular for cluster analysis in data mining. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, Better Euclidean solutions can be found using k-medians and k-medoids.\n",
    "\n",
    "The above algorithm in pseudocode: \n",
    "\n",
    "Initialize k means with random values\n",
    "\n",
    "For a given number of iterations:\n",
    "    \n",
    "Iterate through items\n",
    "\n",
    "Use kmeans with different number of clusters\n",
    "\n",
    "Append the list with kmeans.inertia_ values\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with the optimal k value implement the K-Means Clustering Algorithm\n",
    "\n",
    "\n",
    "# now using kmeans.fit_predict to predict that which data belong to which cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Visualising the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualise the final result use plt.scatter() with respective arguments to view the created clusters\n",
    "\n",
    "\n",
    "# also plot the centroids of the respective clusters using kmeans.cluster_centers_  \n",
    "\n",
    "\n",
    "# finally you will be able to look at the result using plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
